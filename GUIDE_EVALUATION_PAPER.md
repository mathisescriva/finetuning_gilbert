# Guide d'√©valuation compl√®te pour publication

Ce guide explique comment g√©n√©rer toutes les m√©triques n√©cessaires pour votre papier.

## üìä M√©triques mesur√©es

1. **Performance (qualit√©)**:
   - WER (Word Error Rate) sur diff√©rents datasets
   - CER (Character Error Rate)
   - M√©triques par dataset (Common Voice, MLS, etc.)

2. **Performance (vitesse)**:
   - Latence par √©chantillon
   - D√©bit (throughput)
   - M√©moire VRAM utilis√©e

3. **Frugalit√©**:
   - Taille du mod√®le (GB)
   - Nombre de param√®tres
   - M√©moire RAM/VRAM

4. **Comparaison**:
   - vs Whisper Large-v3 (baseline)
   - Acc√©l√©ration (speedup)
   - R√©duction de taille
   - D√©gradation qualit√©

## üöÄ √âtapes d'√©valuation

### 1. √âvaluation compl√®te (mod√®le + baseline)

```bash
cd /workspace/finetuning_gilbert
git pull

# √âvaluation compl√®te (prend 10-30 minutes selon datasets)
python scripts/evaluate_comprehensive.py \
    --model bofenghuang/whisper-large-v3-distil-fr-v0.2 \
    --baseline_model openai/whisper-large-v3 \
    --device cuda \
    --max_samples 100 \
    --output outputs/evaluations/comprehensive_results.json
```

**Options importantes**:
- `--max_samples`: Nombre d'√©chantillons par dataset (d√©faut: 100)
- `--skip_baseline`: Pour skip l'√©valuation baseline (plus rapide)
- `--device`: `cuda` ou `cpu`

### 2. G√©n√©rer les tableaux pour le papier

```bash
# G√©n√©rer tableaux LaTeX et Markdown
python scripts/generate_publication_table.py \
    --results outputs/evaluations/comprehensive_results.json \
    --format both \
    --output outputs/evaluations/publication_table.md
```

### 3. Benchmark vitesse d√©taill√©

```bash
# Benchmark vitesse avec plus de runs pour statistiques robustes
python scripts/benchmark_model.py \
    --model bofenghuang/whisper-large-v3-distil-fr-v0.2 \
    --device cuda \
    --num_runs 20  # Plus de runs = statistiques plus robustes
```

## üìã Structure des r√©sultats

Le fichier JSON contient:

```json
{
  "model_name": "...",
  "model_size_gb": 1.51,
  "num_parameters": 750000000,
  "inference_benchmark": {
    "mean_time": 0.03,
    "std_time": 0.001,
    "peak_memory_gb": 1.57
  },
  "quality_metrics": {
    "common_voice_fr": {
      "wer": 0.05,
      "cer": 0.02,
      "num_samples": 100
    }
  },
  "average_wer": 0.05,
  "average_cer": 0.02,
  "speedup_vs_baseline": 4.2,
  "size_reduction_percent": 50.0,
  "wer_degradation_vs_baseline": 0.01
}
```

## üìÑ Utilisation pour le papier

### M√©triques principales √† mentionner

1. **Efficacit√©**:
   - Taille: X GB (r√©duction de Y% vs baseline)
   - Acc√©l√©ration: Xx plus rapide
   - M√©moire: X GB VRAM

2. **Qualit√©**:
   - WER moyen: X%
   - CER moyen: X%
   - D√©gradation: +X% vs baseline (si applicable)

3. **Comparaison**:
   - Tableau comparatif automatiquement g√©n√©r√©
   - Graphiques possibles avec les donn√©es JSON

### Exemple de section pour papier

```
Le mod√®le Whisper-Large-V3-Distil-French-v0.2 a √©t√© √©valu√© sur 
[datasets] et compar√© √† Whisper Large-v3. Les r√©sultats montrent:

- Taille: 1.51 GB (r√©duction de 50% vs baseline)
- Vitesse: 4.2x plus rapide
- Qualit√©: WER de 5.2% (d√©gradation de 0.8% vs baseline)
- M√©moire: 1.57 GB VRAM

Ces r√©sultats d√©montrent un excellent compromis qualit√©/frugalit√©...
```

## üî¨ Datasets recommand√©s pour publication

Pour un papier robuste, √©valuer sur:

1. **Common Voice French** (standard, g√©n√©raliste)
2. **MLS French** (haute qualit√©, lecture)
3. **VoxPopuli French** (parlementaire, proche r√©unions)
4. **Dataset interne** (si disponible, sp√©cifique r√©unions)

## ‚ö†Ô∏è Notes importantes

- Les r√©sultats varient selon les datasets
- Plus d'√©chantillons = statistiques plus robustes (mais plus long)
- Le baseline (large-v3) est plus lourd √† √©valuer, utilisez `--skip_baseline` pour tests rapides
- Les m√©triques VRAM d√©pendent du GPU utilis√©

## üìä Visualisations (optionnel)

Les donn√©es JSON peuvent √™tre utilis√©es pour cr√©er des graphiques:

```python
import json
import matplotlib.pyplot as plt

with open("outputs/evaluations/comprehensive_results.json") as f:
    results = json.load(f)

# Cr√©er graphiques comparatifs, etc.
```

## üéØ M√©triques sp√©cifiques r√©unions (si donn√©es disponibles)

Si vous avez un dataset de r√©unions:

```python
# Utiliser le script avec votre dataset custom
python scripts/evaluate_comprehensive.py \
    --model bofenghuang/whisper-large-v3-distil-fr-v0.2 \
    --custom_dataset data/test_sets/meetings_test.json \
    --output outputs/evaluations/meetings_evaluation.json
```

## ‚úÖ Checklist pour papier

- [ ] √âvaluation sur au moins 2 datasets publics
- [ ] Comparaison avec baseline (large-v3)
- [ ] M√©triques de vitesse (latence, throughput)
- [ ] M√©triques de qualit√© (WER, CER)
- [ ] M√©triques de frugalit√© (taille, m√©moire)
- [ ] Tableau comparatif g√©n√©r√©
- [ ] Statistiques robustes (suffisamment d'√©chantillons)
- [ ] M√©triques sur donn√©es de r√©unions (si disponible)

