# Guide : Quantization-Aware Training (QAT) pour Whisper

## üéØ Objectif

Am√©liorer les performances d'un mod√®le Whisper apr√®s quantization (int8/int4) en l'entra√Ænant avec fake quantization.

## üìä Complexit√© : Mod√©r√©e ‚úÖ

**Pourquoi c'est faisable :**
- ‚úÖ PyTorch a des outils int√©gr√©s (`torch.quantization`)
- ‚úÖ Optimum/HuggingFace fournit des helpers
- ‚úÖ M√™me infrastructure que fine-tuning classique
- ‚úÖ Pas besoin de nouveaux datasets

**Points d'attention :**
- ‚ö†Ô∏è N√©cessite un peu de compr√©hension de la quantization
- ‚ö†Ô∏è Entra√Ænement plus long (10+ √©poques vs 3-5)
- ‚ö†Ô∏è Int4 n√©cessite impl√©mentation custom (int8 est standard)

## üöÄ Workflow Complet

### √âtape 1 : Entra√Ænement QAT (Optimis√© : 2-4h sur GPU)

```bash
# Entra√Æner avec QAT (int8) - Version optimis√©e
python scripts/train_qat.py \
  --base_model bofenghuang/whisper-large-v3-distil-fr-v0.2 \
  --train_data data/processed/common_voice_fr \
  --eval_data data/processed/common_voice_fr \
  --quantization_type int8 \
  --num_epochs 5 \
  --max_samples 60000 \
  --per_device_batch_size 8 \
  --output_dir outputs/models/whisper-qat-int8

# Ou avec Makefile (param√®tres optimis√©s par d√©faut)
make train-qat-int8
```

**‚è±Ô∏è Temps estim√© : 2-4 heures sur GPU moderne (A100/V100)**

**Ce que fait le script :**
1. Charge le mod√®le v0.2
2. Active fake quantization (simule int8 pendant training)
3. Entra√Æne le mod√®le pour qu'il apprenne √† r√©sister √† la quantization
4. Utilise sous-ensemble optimis√© (60k samples ‚âà 500h) pour acc√©l√©rer
5. Sauvegarde le mod√®le pr√©par√© pour quantization

**Param√®tres optimis√©s par d√©faut :**
- `num_epochs=5` : Suffisant car mod√®le d√©j√† pr√©-entra√Æn√©
- `max_samples=60000` : ~500h de donn√©es (vs 1000h+ complet)
- `batch_size=8` : Plus grand pour GPU, acc√©l√®re training
- **Temps total : 2-4h sur GPU** (vs 6-12h avec param√®tres standard)

### √âtape 2 : Conversion en Mod√®le Quantifi√©

```bash
# Convertir en mod√®le quantifi√© r√©el (ONNX)
python scripts/convert_qat_to_quantized.py \
  --model_path outputs/models/whisper-qat-int8/final \
  --output_path outputs/models/whisper-qat-int8-quantized \
  --quantization_type int8 \
  --format onnx
```

**Ce que fait le script :**
1. Charge le mod√®le QAT entra√Æn√©
2. Convertit en format ONNX
3. Applique la quantization r√©elle (int8)
4. Sauvegarde mod√®le pr√™t pour inf√©rence

### √âtape 3 : √âvaluation sur les M√™mes Corpus

```bash
# √âvaluer sur les corpus de la model card
python scripts/evaluate_qat.py \
  --model_path outputs/models/whisper-qat-int8-quantized \
  --baseline_model bofenghuang/whisper-large-v3-distil-fr-v0.2 \
  --corpora community-v2 mtedx zaion5 zaion6 \
  --test_data data/test_sets/eval_data.json
```

**Ce que fait le script :**
1. √âvalue le mod√®le quantifi√© sur les m√™mes corpus que v0.2
2. Compare avec le baseline (v0.2 non quantifi√©)
3. Calcule la d√©gradation WER
4. G√©n√®re rapport de comparaison

## üìà R√©sultats Attendus

### Objectifs QAT

| M√©trique | Avant QAT (PTQ) | Apr√®s QAT | Objectif |
|----------|-----------------|-----------|----------|
| **WER d√©gradation int8** | 1-3% | <0.5% | ‚úÖ |
| **WER d√©gradation int4** | 3-5% | <2% | ‚úÖ |
| **Taille mod√®le** | 50% | 25% (int8) / 12.5% (int4) | ‚úÖ |
| **Vitesse CPU** | +2x | +3-4x | ‚úÖ |

### Comparaison avec v0.2

Le mod√®le QAT devrait avoir :
- ‚úÖ M√™me WER que v0.2 (avant quantization)
- ‚úÖ D√©gradation <0.5% en int8 (vs 1-3% sans QAT)
- ‚úÖ Utilisable en int4 avec d√©gradation acceptable (<2%)

## üìã Corpus d'√âvaluation

Vous pouvez √©valuer sur **exactement les m√™mes corpus** que dans la model card :

### Corpus Publics

1. **community-v2/dev_data** : Common Voice fran√ßais
2. **mtedx** : MTEDx fran√ßais (lectures TED Talks)
3. **zaion5** : Dataset interne Zaion Lab (call centers)
4. **zaion6** : Dataset interne Zaion Lab (call centers)

### Utilisation

```bash
# √âvaluer sur corpus publics
python scripts/evaluate_qat.py \
  --model_path outputs/models/whisper-qat-int8-quantized \
  --corpora community-v2 mtedx

# √âvaluer sur votre dataset de test
python scripts/evaluate_qat.py \
  --model_path outputs/models/whisper-qat-int8-quantized \
  --test_data data/test_sets/your_test.json
```

## üî¨ D√©tails Techniques

### Fake Quantization

Pendant l'entra√Ænement QAT :
- Les poids et activations sont "fake quantifi√©s" (simul√©s)
- Le mod√®le apprend √† fonctionner avec cette contrainte
- Pas de vraie quantization (on garde float32 pour gradients)

### Conversion Finale

Apr√®s entra√Ænement :
- Conversion en ONNX quantifi√© r√©el
- Int8 : 8 bits par poids/activation
- Int4 : 4 bits (n√©cessite impl√©mentation custom)

## üìä Tableau Comparatif pour Publication

Apr√®s √©valuation, vous aurez :

| Mod√®le | Format | community-v2 | mtedx | zaion5 | zaion6 | Taille | Vitesse |
|--------|--------|--------------|-------|--------|--------|--------|---------|
| v0.2 | float16 | 9.44 | 8.94 | 29.4 | 26.17 | 100% | 1x |
| v0.2 | int8 (PTQ) | 9.8 | 9.2 | 31.0 | 27.5 | 50% | 2x |
| **v0.3-QAT** | **int8** | **9.5** | **9.0** | **29.8** | **26.5** | **50%** | **2x** |
| v0.3-QAT | int4 | 9.7 | 9.3 | 30.5 | 27.0 | 25% | 3-4x |

*(Valeurs exemple - vos r√©sultats peuvent varier)*

## ‚úÖ Avantages pour Publication

1. **Contribution claire** : Premi√®re QAT pour distille Whisper fran√ßais
2. **R√©sultats mesurables** : Comparaison directe avec v0.2 et PTQ
3. **Impact pratique** : D√©ploiement edge/cloud optimis√©
4. **Reproducibilit√©** : Code + datasets publics

## ‚öôÔ∏è Param√®tres Recommand√©s

### QAT Int8 (Param√®tres par D√©faut - Optimis√©) ‚≠ê

**Version par d√©faut** (qualit√© excellente, rapide) :
```yaml
num_epochs: 5  # Suffisant car on part de v0.2 pr√©-entra√Æn√©
learning_rate: 5e-6
batch_size: 8  # Optimal pour GPU
gradient_accumulation: 4
max_samples: 60000  # ~500h de segments 30s
temps_estim√©: 2-4h sur GPU moderne
```

**Version Extended** (qualit√© maximale, plus long) :
```yaml
num_epochs: 10
learning_rate: 5e-6
batch_size: 8
gradient_accumulation: 4
max_samples: 120000  # ~1000h complet
temps_estim√©: 6-10h sur GPU moderne
```

**‚ö†Ô∏è Important** : 
- Comme on part de v0.2 d√©j√† entra√Æn√©, QAT n√©cessite **beaucoup moins d'√©poques** que l'entra√Ænement initial (5 vs 160)
- Pas besoin de tout le dataset : **500h suffisent** (vs 10,000h pour l'entra√Ænement initial)
- **Temps r√©el : 2-4h sur GPU** avec param√®tres optimis√©s (vs plusieurs jours pour entra√Ænement complet)

### QAT Int4

```yaml
num_epochs: 15  # Plus long car plus difficile
learning_rate: 3e-6  # Encore plus bas
batch_size: 2
gradient_accumulation: 16
```

## üêõ Troubleshooting

### Erreur "torch.quantization not available"

**Solution** : Installer PyTorch avec support quantization
```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

### Erreur "ONNX conversion failed"

**Solution** : Utiliser Optimum
```bash
pip install optimum[onnxruntime]
```

### D√©gradation >1% apr√®s QAT

**Causes possibles :**
- Pas assez d'√©poques (augmenter √† 15-20)
- Learning rate trop √©lev√© (r√©duire)
- Fake quantization mal configur√©e

**Solution** : Ajuster hyperparam√®tres et r√©-entra√Æner

## üìù Checklist Publication

- [ ] Entra√Ænement QAT int8 compl√©t√©
- [ ] Conversion en mod√®le quantifi√©
- [ ] √âvaluation sur tous les corpus (community-v2, mtedx, zaion5, zaion6)
- [ ] Comparaison avec v0.2 et PTQ
- [ ] Mesure gains m√©moire/vitesse
- [ ] Documentation code + hyperparam√®tres
- [ ] Publication mod√®les sur HuggingFace

## üöÄ Quick Start

```bash
# 1. Entra√Æner QAT (2-4h sur GPU avec param√®tres optimis√©s)
make train-qat-int8

# 2. Convertir en mod√®le quantifi√© r√©el
python scripts/convert_qat_to_quantized.py \
  --model_path outputs/models/whisper-qat-int8/final \
  --output_path outputs/models/whisper-qat-int8-quantized \
  --quantization_type int8

# 3. √âvaluer (comparaison avec baseline v0.2)
make evaluate-qat
```

**Temps total workflow : ~3-5 heures** (2-4h training + 30min conversion + 30min √©valuation)

---

**En r√©sum√©** : Le QAT est **mod√©r√© en complexit√©** et vous pourrez √©valuer sur **exactement les m√™mes corpus** que v0.2 pour une comparaison directe ! üéØ

