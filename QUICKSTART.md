# Quickstart : Fine-tuning Whisper pour R√©unions

## D√©marrage Rapide

### 1. Installation

```bash
# Cloner ou naviguer dans le projet
cd finetuning_gilbert

# Installer d√©pendances
pip install -r requirements.txt
```

### 2. T√©l√©charger des Donn√©es Publiques üá´üá∑

**Option A : T√©l√©charger automatiquement Common Voice fran√ßais (RECOMMAND√â)**

```bash
# T√©l√©charger Common Voice fran√ßais (dataset public gratuit)
python scripts/download_datasets.py --datasets common_voice

# Ou avec le Makefile
make download-datasets
```

Cela t√©l√©charge ~100+ heures de donn√©es fran√ßaises dans `data/processed/common_voice_fr/`.

**Option B : Utiliser vos propres donn√©es de r√©unions**

Si vous avez vos propres donn√©es, cr√©er un fichier JSON :

```json
[
  {
    "audio": "path/to/meeting1.wav",
    "text": "Transcription de r√©f√©rence de la r√©union..."
  },
  {
    "audio": "path/to/meeting2.wav",
    "text": "Autre transcription..."
  }
]
```

Sauvegarder dans `data/raw/train_data.json` et `data/raw/eval_data.json`.

**Voir `DATASETS.md` pour plus d'options de datasets publics.**

### 3. √âvaluation Baseline

√âvaluer le mod√®le de base pour √©tablir la baseline :

```bash
# Si vous avez t√©l√©charg√© Common Voice
python scripts/evaluate_baseline.py \
  --model_name bofenghuang/whisper-large-v3-distil-fr-v0.2 \
  --test_data data/processed/common_voice_fr \
  --output_dir outputs/evaluations

# Ou avec vos donn√©es JSON
python scripts/evaluate_baseline.py \
  --model_name bofenghuang/whisper-large-v3-distil-fr-v0.2 \
  --test_data data/raw/eval_data.json \
  --output_dir outputs/evaluations
```

### 4. Fine-tuning

#### Phase 1 : Encoder Frozen

**Avec dataset HuggingFace (Common Voice)** :
```bash
python scripts/fine_tune_meetings.py \
  --model_name bofenghuang/whisper-large-v3-distil-fr-v0.2 \
  --train_data data/processed/common_voice_fr \
  --eval_data data/processed/common_voice_fr \
  --output_dir outputs/models/whisper-meetings-phase1 \
  --phase 1
```

**Avec vos donn√©es JSON** :
```bash
python scripts/fine_tune_meetings.py \
  --model_name bofenghuang/whisper-large-v3-distil-fr-v0.2 \
  --train_data data/raw/train_data.json \
  --eval_data data/raw/eval_data.json \
  --output_dir outputs/models/whisper-meetings-phase1 \
  --phase 1
```

#### Phase 2 : Full Fine-tuning

```bash
python scripts/fine_tune_meetings.py \
  --model_name outputs/models/whisper-meetings-phase1/final \
  --train_data data/raw/train_data.json \
  --eval_data data/raw/eval_data.json \
  --output_dir outputs/models/whisper-meetings-phase2 \
  --phase 2
```

#### Phase 3 : LoRA (Optionnel)

```bash
python scripts/fine_tune_meetings.py \
  --model_name bofenghuang/whisper-large-v3-distil-fr-v0.2 \
  --train_data data/raw/train_data.json \
  --eval_data data/raw/eval_data.json \
  --output_dir outputs/models/whisper-meetings-lora \
  --phase 3 \
  --use_lora
```

### 5. Quantization (Optionnel)

Quantifier le mod√®le en int8 pour r√©duire taille et latence :

```bash
python scripts/distill_quantize.py \
  --model_path outputs/models/whisper-meetings-phase2/final \
  --output_dir outputs/models/whisper-meetings-int8 \
  --quantization_type int8
```

### 6. Benchmark Comparatif

Comparer diff√©rents mod√®les :

```bash
python scripts/benchmark.py \
  --test_data data/raw/eval_data.json \
  --models \
    openai/whisper-large-v3 \
    bofenghuang/whisper-large-v3-distil-fr-v0.2 \
  --custom_models \
    outputs/models/whisper-meetings-phase2/final \
  --output_dir outputs/evaluations
```

### 7. Utilisation du Mod√®le

Voir `GUIDE_INTEGRATION.md` pour les d√©tails complets.

**Exemple rapide avec Transformers :**

```python
from transformers import WhisperProcessor, WhisperForConditionalGeneration
import librosa

# Charger mod√®le
model_name = "outputs/models/whisper-meetings-phase2/final"
processor = WhisperProcessor.from_pretrained(model_name)
model = WhisperForConditionalGeneration.from_pretrained(model_name)

# Transcrire
audio, sr = librosa.load("meeting.wav", sr=16000)
inputs = processor(audio, sampling_rate=sr, return_tensors="pt")
with torch.no_grad():
    generated_ids = model.generate(inputs["input_features"], language="fr")
transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(transcription)
```

## Structure des Donn√©es

### Format JSON Attendu

```json
{
  "test_samples": [
    {
      "audio": "path/to/audio.wav",
      "text": "Transcription compl√®te..."
    }
  ],
  "entities": ["Nom1", "Nom2", "ACRONYME"],  // Optionnel
  "metadata": {
    "description": "...",
    "language": "fr"
  }
}
```

### Formats Audio Support√©s

- WAV (recommand√©)
- MP3
- FLAC
- OGG

Sample rate sera automatiquement converti √† 16 kHz.

## Configuration

Modifier `config/training_config.yaml` et `config/model_config.yaml` selon besoins.

### Param√®tres Cl√©s

**Training :**
- `learning_rate` : Taux d'apprentissage (d√©faut: 1e-5)
- `per_device_train_batch_size` : Taille batch (d√©faut: 8)
- `num_epochs` : Nombre d'√©poques (d√©faut: 3-5)

**Inference :**
- `beam_size` : Taille beam search (d√©faut: 5)
- `chunk_length_s` : Longueur chunks (d√©faut: 30)

## Troubleshooting

**Erreur CUDA OOM :**
- R√©duire `per_device_train_batch_size`
- Augmenter `gradient_accumulation_steps`
- Utiliser `fp16: true` dans config

**Qualit√© insuffisante :**
- V√©rifier qualit√© donn√©es (transcriptions pr√©cises)
- Augmenter nombre d'√©poques
- Ajouter plus de donn√©es d'entra√Ænement
- V√©rifier augmentations audio (peuvent √™tre trop agressives)

**Latence √©lev√©e :**
- Utiliser `faster-whisper` au lieu de `transformers`
- R√©duire `beam_size` (3 ou 1)
- Quantifier mod√®le (int8)

## Ressources

- **Plan technique** : `PLAN_TECHNIQUE.md`
- **Guide int√©gration** : `GUIDE_INTEGRATION.md`
- **Limites & next steps** : `LIMITES_ET_NEXT_STEPS.md`

