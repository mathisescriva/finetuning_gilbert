# Solution : Probl√®me de M√©moire (RAM)

## üéØ Probl√®me

`std::bad_alloc` = Plus assez de RAM pour charger 60k √©chantillons en m√©moire.

## ‚úÖ Solutions Simples

### Option 1 : R√©duire le nombre d'√©chantillons (RAPIDE)

Modifier `scripts/train_qat_vast_ai.sh` :

```bash
# Changer cette ligne :
MAX_SAMPLES=60000  # Trop pour la RAM disponible

# En :
MAX_SAMPLES=10000  # Beaucoup plus l√©ger, suffisant pour QAT
```

Puis relancer :

```bash
bash scripts/train_qat_vast_ai.sh
```

**Temps** : ~30-45 min au lieu de 1-2h, mais r√©sultat similaire (QAT fonctionne bien avec moins de donn√©es).

---

### Option 2 : Utiliser PTQ directement (PLUS SIMPLE)

**Pas besoin d'entra√Ænement**, quantization directe :

```bash
python scripts/quantize_ptq.py \
  --model bofenghuang/whisper-large-v3-distil-fr-v0.2 \
  --quantization_type int8 \
  --output_dir outputs/models/gilbert-whisper-ptq-int8
```

**Avantages** :
- ‚úÖ **5-10 minutes** (vs 1-2h)
- ‚úÖ **Pas besoin de dataset**
- ‚úÖ **Pas de probl√®me de m√©moire**
- ‚úÖ **Fonctionne imm√©diatement**

**R√©sultat** :
- Qualit√© : ~1-2% d√©gradation (vs <0.5% avec QAT)
- Taille/vitesse : Identique (50% r√©duction, 2-3x plus rapide)

---

### Option 3 : Utiliser Trainer avec Streaming (Plus complexe)

Le Trainer de HuggingFace peut g√©rer le streaming nativement sans charger tout en m√©moire. Mais cela n√©cessite de modifier le code.

---

## üéØ Recommandation

**Pour arriver rapidement √† votre objectif** : **Option 2 (PTQ)**

Vous obtiendrez un mod√®le quantifi√© en **5-10 minutes** sans probl√®me de m√©moire ou de dataset.

**Si vous voulez le meilleur r√©sultat** : **Option 1** (r√©duire √† 10k √©chantillons)

---

## üìù Commande Rapide PTQ

```bash
cd /workspace/finetuning_gilbert
python scripts/quantize_ptq.py \
  --model bofenghuang/whisper-large-v3-distil-fr-v0.2 \
  --quantization_type int8 \
  --output_dir outputs/models/gilbert-whisper-ptq-int8
```

C'est tout ! En 5-10 minutes vous aurez votre mod√®le quantifi√©. üöÄ

