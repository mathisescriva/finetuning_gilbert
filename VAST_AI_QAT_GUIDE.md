# Guide Complet : QAT sur Vast.ai pour Mod√®le Propri√©taire

## üéØ Objectif

Cr√©er un mod√®le Whisper optimis√© avec **Quantization-Aware Training (QAT)** sur Vast.ai, maximisant :
- ‚úÖ **Performance** : Qualit√© pr√©serv√©e (<0.5% d√©gradation WER)
- ‚úÖ **Frugalit√©** : 2-4x r√©duction m√©moire (int8) ou 4-8x (int4)
- ‚úÖ **Vitesse** : 2-3x plus rapide en inf√©rence

## üìã Pr√©requis

1. **Compte Vast.ai** : https://vast.ai
2. **SSH Key** : Cl√© SSH configur√©e sur Vast.ai
3. **Repo Git** : Votre projet sur GitHub/GitLab (ou upload manuel)

## üöÄ Setup Rapide (5 minutes)

### √âtape 1 : Cr√©er Instance sur Vast.ai

1. Aller sur https://vast.ai
2. **Create** ‚Üí **GPU Instance**
3. **S√©lectionner GPU** :
   - **Recommand√©** : RTX 3090, RTX 4090, ou A100 (16GB+ VRAM)
   - **Budget** : RTX 3060 12GB (plus lent mais fonctionne)
   - **Co√ªt** : ~$0.20-0.50/h selon GPU
4. **Template** : PyTorch (ou Ubuntu + CUDA)
5. **Disk Space** : Minimum 100GB (recommand√© 200GB+)
6. **Cr√©er l'instance**

### √âtape 2 : Se Connecter en SSH

```bash
# R√©cup√©rer la commande SSH depuis Vast.ai (dans "Connect")
# Format typique :
ssh root@ssh4.vast.ai -p <PORT> -i ~/.ssh/id_ed25519
```

### √âtape 3 : Setup Automatique

Une fois connect√© sur Vast.ai, ex√©cuter :

```bash
# Cloner le repo
cd /workspace
git clone <votre-repo-url> finetuning_gilbert
cd finetuning_gilbert

# Lancer setup automatique
bash scripts/setup_vast_ai_qat.sh
```

**C'est tout !** Le script fait :
- ‚úÖ Installation d√©pendances
- ‚úÖ Configuration environnement
- ‚úÖ T√©l√©chargement datasets (si n√©cessaire)
- ‚úÖ Lancement entra√Ænement QAT optimis√©

## üìä Configuration Optimis√©e

### Param√®tres pour Performance/Frugalit√©/Vitesse

Le script utilise ces param√®tres optimis√©s :

```yaml
# Performance (qualit√©)
- num_epochs: 5  # Suffisant car mod√®le d√©j√† pr√©-entra√Æn√©
- learning_rate: 5e-6  # Conservateur pour pr√©server qualit√©
- max_samples: 60000  # ~500h (vs 1000h+ complet)

# Frugalit√© (m√©moire)
- per_device_batch_size: 8  # Optimis√© pour GPU 16-24GB
- gradient_accumulation_steps: 4  # √âquivalent batch_size 32
- fp16: true  # R√©duit m√©moire de 50%

# Vitesse (inf√©rence)
- quantization_type: int8  # 2-3x plus rapide que float16
- format: onnx  # Optimis√© pour inf√©rence
```

## üîß Scripts Disponibles

### 1. Setup Automatique

```bash
bash scripts/setup_vast_ai_qat.sh
```

**Fait** :
- V√©rifie GPU et espace disque
- Installe d√©pendances (transformers, optimum, etc.)
- Configure cache HuggingFace sur `/workspace` (plus d'espace)
- Pr√©pare environnement

### 2. Entra√Ænement QAT Optimis√©

```bash
# Option A : Script automatique (recommand√©)
bash scripts/train_qat_vast_ai.sh

# Option B : Commande manuelle
python scripts/train_qat_optimized.py \
  --base_model bofenghuang/whisper-large-v3-distil-fr-v0.2 \
  --quantization_type int8 \
  --output_dir outputs/models/gilbert-whisper-qat-int8 \
  --max_samples 60000 \
  --num_epochs 5 \
  --per_device_batch_size 8
```

### 3. Conversion en Mod√®le Quantifi√©

Apr√®s entra√Ænement QAT :

```bash
python scripts/convert_qat_to_quantized.py \
  --model_path outputs/models/gilbert-whisper-qat-int8/final \
  --output_path outputs/models/gilbert-whisper-qat-int8-quantized \
  --quantization_type int8 \
  --format onnx
```

### 4. Benchmark Performance

```bash
python scripts/benchmark_quantized.py \
  --model_path outputs/models/gilbert-whisper-qat-int8-quantized \
  --device cuda \
  --num_runs 20
```

## ‚è±Ô∏è Temps Estim√©

| √âtape | Temps | Description |
|-------|-------|-------------|
| **Setup** | 5-10 min | Installation d√©pendances |
| **T√©l√©chargement datasets** | 10-30 min | Si pas d√©j√† t√©l√©charg√©s |
| **Entra√Ænement QAT** | **2-4h** | Sur GPU moderne (RTX 3090+) |
| **Conversion quantifi√©e** | 5-10 min | ONNX + quantization |
| **Benchmark** | 5 min | Tests performance |
| **Total** | **3-5h** | De bout en bout |

**Co√ªt estim√©** : $0.60-2.00 (selon GPU et dur√©e)

## üìà R√©sultats Attendus

### M√©triques Cibles

| M√©trique | Baseline (v0.2) | QAT int8 | Am√©lioration |
|----------|----------------|----------|--------------|
| **WER** | R√©f√©rence | +0.3-0.5% | ‚úÖ Minimal |
| **Taille** | 1.51 GB | **0.75 GB** | ‚úÖ **-50%** |
| **VRAM** | 1.57 GB | **0.8 GB** | ‚úÖ **-49%** |
| **Vitesse** | Baseline | **2-3x** | ‚úÖ **+200%** |
| **Latence** | 0.053s | **0.02-0.03s** | ‚úÖ **-40%** |

### Comparaison Formats

| Format | Taille | VRAM | Vitesse | Qualit√© |
|--------|--------|------|---------|---------|
| **FP16** (baseline) | 1.51 GB | 1.57 GB | 1x | 100% |
| **int8 (QAT)** | 0.75 GB | 0.8 GB | 2-3x | 99.5% |
| **int4 (QAT)** | 0.38 GB | 0.4 GB | 4-5x | 98% |

## üéØ Workflow Complet

### Phase 1 : Setup (10 min)

```bash
# Sur Vast.ai
cd /workspace
git clone <votre-repo> finetuning_gilbert
cd finetuning_gilbert
bash scripts/setup_vast_ai_qat.sh
```

### Phase 2 : Entra√Ænement QAT (2-4h)

```bash
# Lancer entra√Ænement (peut tourner en arri√®re-plan)
nohup bash scripts/train_qat_vast_ai.sh > training.log 2>&1 &

# Suivre les logs
tail -f training.log
```

### Phase 3 : Conversion (10 min)

```bash
# Apr√®s entra√Ænement termin√©
python scripts/convert_qat_to_quantized.py \
  --model_path outputs/models/gilbert-whisper-qat-int8/final \
  --output_path outputs/models/gilbert-whisper-qat-int8-quantized \
  --quantization_type int8
```

### Phase 4 : √âvaluation (15 min)

```bash
# Benchmark performance
python scripts/benchmark_quantized.py \
  --model_path outputs/models/gilbert-whisper-qat-int8-quantized

# √âvaluation qualit√© (WER/CER)
python scripts/evaluate_wer.py \
  --model outputs/models/gilbert-whisper-qat-int8-quantized \
  --dataset facebook/multilingual_librispeech \
  --dataset_config french \
  --split test \
  --max_samples 100
```

### Phase 5 : Sauvegarde (5 min)

```bash
# Option A : Upload vers HuggingFace (recommand√©)
huggingface-cli login
huggingface-cli upload <votre-username>/gilbert-whisper-qat-int8 \
  outputs/models/gilbert-whisper-qat-int8-quantized

# Option B : T√©l√©charger localement
# Depuis votre machine locale
scp -r root@<vast-ip>:/workspace/finetuning_gilbert/outputs/models/gilbert-whisper-qat-int8-quantized ./
```

## üîç Monitoring

### Pendant l'Entra√Ænement

```bash
# Voir logs en temps r√©el
tail -f training.log

# V√©rifier utilisation GPU
watch -n 1 nvidia-smi

# V√©rifier espace disque
df -h /workspace
```

### M√©triques √† Surveiller

- **Loss** : Doit diminuer progressivement
- **WER (eval)** : Doit rester proche du baseline (<1% d√©gradation)
- **GPU Utilisation** : Doit √™tre >80% pendant training
- **VRAM** : Ne pas d√©passer capacit√© GPU

## üêõ Troubleshooting

### Probl√®me : "Out of Memory"

**Solution** :
```bash
# R√©duire batch size
python scripts/train_qat_optimized.py \
  --per_device_batch_size 4 \  # Au lieu de 8
  --gradient_accumulation_steps 8  # Compenser
```

### Probl√®me : "No space left on device"

**Solution** :
```bash
# Nettoyer cache
bash scripts/cleanup_disk.sh

# Utiliser /workspace pour cache HuggingFace
export HF_HOME=/workspace/.hf_home
export TRANSFORMERS_CACHE=/workspace/.hf_home
```

### Probl√®me : "CUDA out of memory"

**Solution** :
```bash
# R√©duire batch size et activer gradient checkpointing
python scripts/train_qat_optimized.py \
  --per_device_batch_size 2 \
  --gradient_checkpointing
```

## üìù Notes Importantes

### Pour Mod√®le Propri√©taire

1. **Nom du mod√®le** : Utiliser `gilbert-whisper-qat-int8` (ou votre nom)
2. **Licence** : Sp√©cifier dans model card (MIT si bas√© sur v0.2)
3. **Cr√©dits** : Mentionner base `bofenghuang/whisper-large-v3-distil-fr-v0.2`

### Optimisations Incluses

- ‚úÖ **FP16 training** : R√©duit m√©moire
- ‚úÖ **Gradient accumulation** : Simule batch size plus grand
- ‚úÖ **Optimized datasets** : Sous-ensemble pour acc√©l√©rer
- ‚úÖ **ONNX export** : Format optimis√© inf√©rence
- ‚úÖ **Cache management** : Utilise `/workspace` (plus d'espace)

## üéì Prochaines √âtapes

Apr√®s QAT r√©ussi :

1. **Publier sur HuggingFace** : Mod√®le quantifi√© pr√™t √† l'emploi
2. **Benchmark complet** : Comparer avec baseline sur tous datasets
3. **Documentation** : Cr√©er model card avec m√©triques
4. **D√©ploiement** : Int√©grer dans votre application

## üí° Astuces

- **Sauvegarder checkpoints** : Le script sauvegarde automatiquement
- **Resume training** : Si interrompu, peut reprendre depuis checkpoint
- **Multi-GPU** : Si disponible, activer avec `--num_gpus`
- **TensorBoard** : Logs disponibles dans `outputs/logs/`

---

**Questions ?** Voir `GUIDE_QAT.md` pour d√©tails techniques.

