#!/usr/bin/env python3
"""
Post-Training Quantization (PTQ) pour Whisper.
Simple et rapide - pas besoin de donn√©es d'entra√Ænement.
"""

import argparse
import torch
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor
from optimum.onnxruntime import ORTModelForSpeechSeq2Seq
from optimum.onnxruntime.configuration import AutoQuantizationConfig
from optimum.onnxruntime import ORTQuantizer
from pathlib import Path
import os


def quantize_to_int8(model_name_or_path: str, output_path: str):
    """
    Quantifie un mod√®le Whisper en int8 avec PTQ.
    """
    print("üîß Post-Training Quantization (PTQ) pour Whisper")
    print(f"üì• Mod√®le source: {model_name_or_path}")
    print(f"üíæ Mod√®le de sortie: {output_path}")
    print()
    
    # Changer le cache HuggingFace vers /workspace (plus d'espace)
    os.environ["HF_HOME"] = "/workspace/.hf_home"
    os.environ["TRANSFORMERS_CACHE"] = "/workspace/.hf_home/hub"
    os.environ["HF_DATASETS_CACHE"] = "/workspace/.hf_home/datasets"
    
    output_path = Path(output_path)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Charger le mod√®le et le processeur
    print("üì¶ Chargement du mod√®le...")
    processor = AutoProcessor.from_pretrained(model_name_or_path)
    model = AutoModelForSpeechSeq2Seq.from_pretrained(
        model_name_or_path,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
    )
    
    # Sauvegarder le processeur
    processor.save_pretrained(output_path)
    
    print("‚úÖ Mod√®le charg√©")
    print(f"üìä Taille avant quantization: {sum(p.numel() * 4 for p in model.parameters()) / 1e9:.2f} GB (float32)")
    print()
    
    # Exporter vers ONNX d'abord
    print("üîÑ Export ONNX...")
    onnx_model_path = output_path / "onnx"
    onnx_model_path.mkdir(exist_ok=True)
    
    try:
        # Exporter avec optimum
        print("  Exportation du mod√®le vers ONNX...")
        onnx_model = ORTModelForSpeechSeq2Seq.from_pretrained(
            model_name_or_path,
            export=True,
            use_cache=False,
        )
        onnx_model.save_pretrained(str(onnx_model_path))
        print("  ‚úÖ Export ONNX r√©ussi")
        
        # Quantifier
        print("üî¢ Quantification int8...")
        quantizer = ORTQuantizer.from_pretrained(onnx_model_path)
        
        # Configuration quantization dynamic (pas besoin de calibration data)
        qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False)
        
        print("  Application de la quantization...")
        quantizer.quantize(
            save_dir=str(output_path / "quantized"),
            quantization_config=qconfig,
        )
        
        print()
        print("‚úÖ ‚úÖ ‚úÖ QUANTIZATION TERMIN√âE! ‚úÖ ‚úÖ ‚úÖ")
        print(f"üìÅ Mod√®le quantifi√© dans: {output_path / 'quantized'}")
        print()
        print("üí° Utilisation:")
        print(f"   from optimum.onnxruntime import ORTModelForSpeechSeq2Seq")
        print(f"   model = ORTModelForSpeechSeq2Seq.from_pretrained('{output_path / 'quantized'}')")
        print()
        
        # Estimation taille
        if (output_path / "quantized").exists():
            total_size = sum(
                f.stat().st_size 
                for f in (output_path / "quantized").rglob("*") 
                if f.is_file()
            ) / 1e9
            original_size = sum(p.numel() * 4 for p in model.parameters()) / 1e9
            reduction = (1 - total_size / original_size) * 100
            print(f"üìä Taille apr√®s quantization: ~{total_size:.2f} GB (int8)")
            print(f"üíæ R√©duction: ~{reduction:.1f}%")
        
    except Exception as e:
        print(f"‚ùå Erreur lors de l'export/quantization: {e}")
        print()
        print("üí° Alternative: Quantization PyTorch native (moins optimis√© mais plus simple)")
        
        # Alternative: quantization PyTorch native
        try:
            print("\nüîÑ Tentative avec quantization PyTorch native...")
            model_quantized = torch.quantization.quantize_dynamic(
                model,
                {torch.nn.Linear},
                dtype=torch.qint8
            )
            
            quantized_path = output_path / "quantized_pytorch"
            quantized_path.mkdir(exist_ok=True)
            model_quantized.save_pretrained(str(quantized_path))
            processor.save_pretrained(str(quantized_path))
            
            print(f"‚úÖ Mod√®le quantifi√© PyTorch sauvegard√© dans: {quantized_path}")
            print("   (Moins optimis√© que ONNX mais fonctionne)")
            
        except Exception as e2:
            print(f"‚ùå Erreur avec PyTorch quantization: {e2}")
            raise


def main():
    parser = argparse.ArgumentParser(
        description="Post-Training Quantization (PTQ) pour Whisper - Simple et rapide"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="bofenghuang/whisper-large-v3-distil-fr-v0.2",
        help="Mod√®le HuggingFace √† quantifier",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="outputs/models/whisper-ptq-int8",
        help="R√©pertoire de sortie",
    )
    
    args = parser.parse_args()
    
    quantize_to_int8(args.model, args.output)


if __name__ == "__main__":
    main()

