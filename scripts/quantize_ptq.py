#!/usr/bin/env python3
"""
Post-Training Quantization (PTQ) pour Whisper.
Simple et rapide - pas besoin de donn√©es d'entra√Ænement.
"""

import argparse
import os
import gc
import torch
import shutil
from pathlib import Path
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor
from optimum.onnxruntime import ORTModelForSpeechSeq2Seq
from optimum.onnxruntime.configuration import AutoQuantizationConfig
from optimum.onnxruntime import ORTQuantizer


def quantize_to_int8(model_name_or_path: str, output_path: str):
    """
    Quantifie un mod√®le Whisper en int8 avec PTQ.
    """
    print("üîß Post-Training Quantization (PTQ) pour Whisper")
    print(f"üì• Mod√®le source: {model_name_or_path}")
    print(f"üíæ Mod√®le de sortie: {output_path}")
    print()
    
    # Changer le cache HuggingFace vers /workspace (plus d'espace)
    # NETTOYER AVANT de t√©l√©charger
    cache_dir = "/workspace/.hf_home"
    if os.path.exists(cache_dir):
        # Supprimer seulement les anciens t√©l√©chargements
        for item in os.listdir(cache_dir):
            item_path = os.path.join(cache_dir, item)
            if os.path.isdir(item_path):
                # Garder seulement le mod√®le qu'on va t√©l√©charger
                if "whisper-large-v3-distil-fr-v0.2" not in item:
                    try:
                        shutil.rmtree(item_path)
                        print(f"  Supprim√©: {item}")
                    except:
                        pass
    
    os.environ["HF_HOME"] = "/workspace/.hf_home"
    os.environ["TRANSFORMERS_CACHE"] = "/workspace/.hf_home/hub"
    os.environ["HF_DATASETS_CACHE"] = "/workspace/.hf_home/datasets"
    
    output_path = Path(output_path)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Charger le mod√®le et le processeur
    print("üì¶ Chargement du mod√®le...")
    processor = AutoProcessor.from_pretrained(model_name_or_path)
    model = AutoModelForSpeechSeq2Seq.from_pretrained(
        model_name_or_path,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
    )
    
    # Sauvegarder le processeur
    processor.save_pretrained(output_path)
    
    print("‚úÖ Mod√®le charg√©")
    # Sauvegarder taille originale avant suppression
    original_size = sum(p.numel() * 4 for p in model.parameters()) / 1e9
    print(f"üìä Taille avant quantization: {original_size:.2f} GB (float32)")
    print()
    
    # Exporter et quantifier avec optimum (m√©thode simplifi√©e)
    print("üîÑ Export et Quantification ONNX...")
    quantized_path = output_path / "quantized"
    quantized_path.mkdir(exist_ok=True)
    
    try:
        # M√©thode 1: Export ONNX puis quantifier avec optimum (g√®re multi-fichiers)
        print("  Exportation ONNX avec quantization int√©gr√©e...")
        
        # Export ONNX standard
        onnx_model_path = output_path / "onnx"
        onnx_model = ORTModelForSpeechSeq2Seq.from_pretrained(
            model_name_or_path,
            export=True,
            use_cache=False,
        )
        onnx_model.save_pretrained(str(onnx_model_path))
        print("  ‚úÖ Export ONNX r√©ussi")
        
        # Lib√©rer m√©moire PyTorch (mais garder fichiers ONNX pour quantification)
        print("  üßπ Lib√©ration m√©moire PyTorch...")
        del model, onnx_model  # Lib√©rer m√©moire GPU/RAM
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print("  ‚úÖ M√©moire lib√©r√©e (fichiers ONNX conserv√©s pour quantification)")
        
        # Quantifier chaque composant s√©par√©ment (encoder, decoder)
        print("üî¢ Quantification int8 (multi-fichiers)...")
        
        # Configuration quantization dynamique (compatible avec ONNX Runtime standard)
        # Utiliser AutoQuantizationConfig avec activations en float32 (pas de ConvInteger)
        qconfig = AutoQuantizationConfig.avx512_vnni(
            is_static=False,  # Dynamic quantization (pas besoin de calibration)
            per_channel=False,  # Plus simple et compatible
        )
        
        # Note: AutoQuantizationConfig avec is_static=False utilise QDQ par d√©faut
        print("  Configuration: Dynamic quantization (compatible ONNX Runtime)")
        
        # Lister les fichiers ONNX
        onnx_files = list(onnx_model_path.glob("*.onnx"))
        print(f"  Trouv√© {len(onnx_files)} fichiers ONNX √† quantifier")
        
        # Quantifier chaque fichier
        quantized_files = {}
        for onnx_file in onnx_files:
            try:
                print(f"  Quantification de {onnx_file.name}...")
                quantizer = ORTQuantizer.from_pretrained(str(onnx_model_path), file_name=onnx_file.name)
                
                # Cr√©er r√©pertoire temporaire pour ce fichier
                temp_quant_dir = output_path / f"temp_quant_{onnx_file.stem}"
                temp_quant_dir.mkdir(exist_ok=True)
                
                quantizer.quantize(
                    save_dir=str(temp_quant_dir),
                    quantization_config=qconfig,
                )
                
                # D√©placer le fichier quantifi√©
                quantized_file = list(temp_quant_dir.glob("*.onnx"))[0]
                target_file = quantized_path / quantized_file.name
                quantized_file.rename(target_file)
                quantized_files[onnx_file.name] = target_file.name
                
                # Nettoyer
                shutil.rmtree(temp_quant_dir)
                print(f"    ‚úÖ {onnx_file.name} quantifi√©")
                
            except Exception as e_file:
                print(f"    ‚ö†Ô∏è  Erreur pour {onnx_file.name}: {e_file}")
                continue
        
        # Copier les autres fichiers n√©cessaires (config, tokenizer, etc.)
        # Mais sauter les fichiers .onnx_data qui sont tr√®s gros
        print("  Copie des fichiers de configuration...")
        skipped = []
        for file in onnx_model_path.glob("*"):
            if file.is_file() and file.suffix != ".onnx" and not file.name.endswith(".onnx_data"):
                try:
                    shutil.copy2(file, quantized_path / file.name)
                except OSError as e:
                    if "No space" in str(e):
                        skipped.append(file.name)
                        print(f"    ‚ö†Ô∏è  Espace insuffisant pour copier {file.name}, cr√©ation lien symbolique...")
                        try:
                            (quantized_path / file.name).symlink_to(file)
                        except:
                            pass
                    else:
                        raise
        
        if skipped:
            print(f"    ‚ö†Ô∏è  {len(skipped)} fichiers non copi√©s (espace insuffisant), utilisent liens symboliques")
        
        # Les fichiers .onnx_data sont d√©j√† r√©f√©renc√©s par les fichiers .onnx quantifi√©s si besoin
        
        # Sauvegarder aussi le processor
        processor.save_pretrained(str(quantized_path))
        
        print()
        print("‚úÖ ‚úÖ ‚úÖ QUANTIZATION TERMIN√âE! ‚úÖ ‚úÖ ‚úÖ")
        print(f"üìÅ Mod√®le quantifi√© dans: {quantized_path}")
        print()
        print("üí° Utilisation:")
        print(f"   from optimum.onnxruntime import ORTModelForSpeechSeq2Seq")
        print(f"   model = ORTModelForSpeechSeq2Seq.from_pretrained('{quantized_path}')")
        print()
        
        # Estimation taille
        if quantized_path.exists():
            total_size = sum(
                f.stat().st_size 
                for f in quantized_path.rglob("*") 
                if f.is_file()
            ) / 1e9
            reduction = (1 - total_size / original_size) * 100 if original_size > 0 else 0
            print(f"üìä Taille apr√®s quantization: ~{total_size:.2f} GB (int8 QDQ)")
            print(f"üìä Taille originale: ~{original_size:.2f} GB (float32)")
            if reduction > 0:
                print(f"üíæ R√©duction: ~{reduction:.1f}%")
            else:
                change = ((total_size - original_size) / original_size) * 100
                print(f"üíæ Augmentation: ~{abs(change):.1f}% (format QDQ peut √™tre plus volumineux, mais inf√©rence plus rapide)")
            print(f"‚ö° Note: QDQ est optimis√© pour vitesse d'inf√©rence, pas pour taille")
        
        # Nettoyer fichiers ONNX non quantifi√©s APR√àS quantification r√©ussie
        print()
        print("  üßπ Nettoyage fichiers temporaires...")
        for onnx_data_file in onnx_model_path.glob("*.onnx_data"):
            try:
                onnx_data_file.unlink()
                print(f"    Supprim√©: {onnx_data_file.name}")
            except:
                pass
        # Garder les .onnx originaux pour r√©f√©rence (petits fichiers)
        print("  ‚úÖ Nettoyage termin√©")
        
    except Exception as e:
        print(f"‚ùå Erreur lors de l'export/quantization: {e}")
        import traceback
        traceback.print_exc()
        print()
        print("üí° Le mod√®le ONNX a √©t√© export√© dans:", onnx_model_path)
        print("   Vous pouvez l'utiliser directement ou quantifier manuellement.")
        raise


def main():
    parser = argparse.ArgumentParser(
        description="Post-Training Quantization (PTQ) pour Whisper - Simple et rapide"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="bofenghuang/whisper-large-v3-distil-fr-v0.2",
        help="Mod√®le HuggingFace √† quantifier",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="outputs/models/whisper-ptq-int8",
        help="R√©pertoire de sortie",
    )
    
    args = parser.parse_args()
    
    quantize_to_int8(args.model, args.output)


if __name__ == "__main__":
    main()

