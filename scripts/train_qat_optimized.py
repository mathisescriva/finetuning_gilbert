#!/usr/bin/env python3
"""
Script d'entra√Ænement QAT optimis√© pour Vast.ai
Focus: Performance/Frugalit√©/Vitesse maximale
"""

import argparse
import yaml
import json
import os
from pathlib import Path
import torch
import torch.nn as nn
from transformers import (
    WhisperProcessor,
    WhisperForConditionalGeneration,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
)
from datasets import load_dataset
import sys

sys.path.append(str(Path(__file__).parent.parent))

from src.data.dataset import prepare_dataset, create_meetings_dataset_from_files
from src.data.augmentations import create_augmentation_pipeline
from src.evaluation.metrics import compute_wer
from src.training.trainer import DataCollatorSpeechSeq2SeqWithPadding

# Configurer temp directory (important pour Vast.ai)
os.environ["TMPDIR"] = os.environ.get("TMPDIR", "/workspace/tmp")
os.environ["TEMP"] = os.environ.get("TEMP", "/workspace/tmp")
os.environ["TMP"] = os.environ.get("TMP", "/workspace/tmp")
os.makedirs(os.environ["TMPDIR"], exist_ok=True)


class QATModelWrapper(nn.Module):
    """Wrapper pour fake quantization pendant training."""
    
    def __init__(self, model, quantization_config=None):
        super().__init__()
        self.model = model
        
        if quantization_config is None:
            quantization_config = {
                "activation": "int8",
                "weight": "int8",
            }
        self.quantization_config = quantization_config
        self._prepare_fake_quantization()
    
    def _prepare_fake_quantization(self):
        """Pr√©pare le mod√®le pour fake quantization."""
        try:
            from torch.quantization import prepare_qat, get_default_qat_qconfig
            
            qconfig = get_default_qat_qconfig('fbgemm')
            
            if self.quantization_config.get("weight") == "int4":
                print("‚ö†Ô∏è  Int4 n√©cessite impl√©mentation custom, utilisation int8")
            
            self.model.train()
            self.model.qconfig = qconfig
            
            try:
                prepare_qat(self.model, inplace=True)
                print("‚úÖ Fake quantization activ√©e")
            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur QAT standard: {e}")
                print("   Utilisation m√©thode alternative")
                self.use_manual_quant = True
        
        except ImportError:
            print("‚ö†Ô∏è  torch.quantization non disponible, m√©thode alternative")
            self.use_manual_quant = True
    
    def forward(self, *args, **kwargs):
        return self.model(*args, **kwargs)


def prepare_qat_model(base_model_path: str, quantization_type: str = "int8"):
    """Pr√©pare un mod√®le pour QAT."""
    print(f"üì¶ Chargement mod√®le: {base_model_path}")
    model = WhisperForConditionalGeneration.from_pretrained(
        base_model_path,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
    )
    
    print(f"üîß Pr√©paration QAT ({quantization_type})...")
    qat_model = QATModelWrapper(
        model,
        quantization_config={
            "activation": quantization_type,
            "weight": quantization_type,
        }
    )
    
    return qat_model.model


def compute_metrics_qat(pred, processor, metric_key_prefix: str = "eval"):
    """Calcule WER comme m√©trique."""
    pred_ids = pred.predictions
    label_ids = pred.label_ids
    
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id
    
    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)
    
    wer = compute_wer(label_str, pred_str)
    
    return {f"{metric_key_prefix}_wer": wer}


def main():
    parser = argparse.ArgumentParser(description="Entra√Ænement QAT optimis√©")
    parser.add_argument(
        "--base_model",
        type=str,
        default="bofenghuang/whisper-large-v3-distil-fr-v0.2",
        help="Mod√®le de base",
    )
    parser.add_argument(
        "--train_data",
        type=str,
        required=True,
        help="Donn√©es d'entra√Ænement",
    )
    parser.add_argument(
        "--eval_data",
        type=str,
        required=True,
        help="Donn√©es d'√©valuation",
    )
    parser.add_argument(
        "--quantization_type",
        type=str,
        choices=["int8", "int4"],
        default="int8",
        help="Type de quantization",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="outputs/models/gilbert-whisper-qat-int8",
        help="R√©pertoire de sortie",
    )
    parser.add_argument(
        "--num_epochs",
        type=int,
        default=5,
        help="Nombre d'√©poques (5 optimis√© pour vitesse)",
    )
    parser.add_argument(
        "--max_samples",
        type=int,
        default=60000,
        help="Limiter dataset (~500h, optimis√© pour vitesse)",
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=5e-6,
        help="Learning rate (conservateur pour qualit√©)",
    )
    parser.add_argument(
        "--per_device_batch_size",
        type=int,
        default=8,
        help="Batch size (8 optimis√© pour GPU 16-24GB)",
    )
    parser.add_argument(
        "--gradient_accumulation_steps",
        type=int,
        default=4,
        help="Gradient accumulation (√©quivalent batch_size 32)",
    )
    
    args = parser.parse_args()
    
    # Ajuster output_dir
    if args.quantization_type not in args.output_dir:
        args.output_dir = args.output_dir.replace("qat", f"qat-{args.quantization_type}")
    
    # Device
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"üéÆ Device: {device}")
    if device == "cpu":
        print("‚ö†Ô∏è  QAT sur CPU sera tr√®s lent. GPU recommand√©.")
    
    # Charger mod√®le et processor
    print(f"üì¶ Chargement mod√®le {args.base_model}...")
    processor = WhisperProcessor.from_pretrained(
        args.base_model,
        language="fr",
        task="transcribe",
    )
    
    # Pr√©parer mod√®le pour QAT
    model = prepare_qat_model(args.base_model, args.quantization_type)
    model = model.to(device)
    
    # Charger donn√©es
    print(f"üìä Chargement donn√©es...")
    
    # Support JSON ou HuggingFace
    if args.train_data.endswith('.json'):
        with open(args.train_data, 'r') as f:
            train_data = json.load(f)
        audio_files = [item["audio"] for item in train_data]
        transcripts = [item["text"] for item in train_data]
        
        train_dataset = create_meetings_dataset_from_files(
            audio_files,
            transcripts,
            processor,
            augmentations=create_augmentation_pipeline({}),
        )
        
        # Limiter si n√©cessaire
        if args.max_samples and len(train_dataset) > args.max_samples:
            train_dataset = train_dataset.select(range(args.max_samples))
    else:
        # HuggingFace dataset - Support MLS format
        try:
            # Essayer de charger avec config (pour MLS: facebook/multilingual_librispeech + french)
            if "multilingual_librispeech" in args.train_data:
                print(f"   Chargement MLS fran√ßais...")
                dataset_full = load_dataset(
                    args.train_data, 
                    "french", 
                    split="train", 
                    streaming=False
                )
            else:
                dataset_full = load_dataset(args.train_data, split="train", streaming=False)
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur chargement dataset: {e}")
            print(f"   Tentative avec split='train' uniquement...")
            dataset_full = load_dataset(args.train_data, split="train", streaming=False)
        
        if args.max_samples and len(dataset_full) > args.max_samples:
            print(f"   Limitation √† {args.max_samples} √©chantillons")
            dataset_full = dataset_full.select(range(args.max_samples))
        
        # Utiliser prepare_dataset pour transformation
        try:
            train_dataset = prepare_dataset(
                args.train_data,
                processor,
                split="train",
                augmentations=create_augmentation_pipeline({}),
            )
            if hasattr(train_dataset, 'dataset') and len(train_dataset.dataset) > args.max_samples:
                train_dataset.dataset = train_dataset.dataset.select(range(args.max_samples))
        except Exception as e:
            print(f"‚ö†Ô∏è  prepare_dataset √©chou√©, utilisation dataset brut: {e}")
            # Fallback: utiliser dataset directement
            train_dataset = dataset_full
    
    # Eval dataset (m√™me logique)
    if args.eval_data.endswith('.json'):
        with open(args.eval_data, 'r') as f:
            eval_data = json.load(f)
        eval_audio_files = [item["audio"] for item in eval_data]
        eval_transcripts = [item["text"] for item in eval_data]
        
        eval_dataset = create_meetings_dataset_from_files(
            eval_audio_files,
            eval_transcripts,
            processor,
            augmentations=None,
        )
    else:
        try:
            # Essayer split validation, sinon dev (pour MLS)
            try:
                if "multilingual_librispeech" in args.eval_data:
                    eval_dataset_raw = load_dataset(
                        args.eval_data, 
                        "french", 
                        split="dev", 
                        streaming=False
                    )
                else:
                    eval_dataset_raw = load_dataset(args.eval_data, split="validation", streaming=False)
            except:
                eval_dataset_raw = load_dataset(args.eval_data, split="dev", streaming=False)
            
            # Limiter taille eval
            if len(eval_dataset_raw) > 1000:
                eval_dataset_raw = eval_dataset_raw.select(range(1000))
            
            # Utiliser prepare_dataset si possible
            try:
                eval_dataset = prepare_dataset(
                    args.eval_data,
                    processor,
                    split="dev" if "multilingual_librispeech" in args.eval_data else "validation",
                    augmentations=None,
                )
                if hasattr(eval_dataset, 'dataset') and len(eval_dataset.dataset) > 1000:
                    eval_dataset.dataset = eval_dataset.dataset.select(range(1000))
            except Exception as e:
                print(f"‚ö†Ô∏è  prepare_dataset eval √©chou√©, utilisation dataset brut: {e}")
                eval_dataset = eval_dataset_raw
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur chargement eval dataset: {e}")
            # Utiliser un sous-ensemble du train comme eval
            print("   Utilisation sous-ensemble train comme eval")
            eval_dataset = dataset_full.select(range(min(1000, len(dataset_full))))
    
    # Data collator
    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)
    
    # Arguments d'entra√Ænement optimis√©s
    training_args = Seq2SeqTrainingArguments(
        output_dir=args.output_dir,
        per_device_train_batch_size=args.per_device_batch_size,
        per_device_eval_batch_size=args.per_device_batch_size * 2,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        warmup_steps=200,
        num_train_epochs=args.num_epochs,
        evaluation_strategy="steps",
        eval_steps=1000,
        save_strategy="steps",
        save_steps=2000,
        logging_steps=50,
        load_best_model_at_end=True,
        metric_for_best_model="eval_wer",
        greater_is_better=False,
        push_to_hub=False,
        report_to=["tensorboard"],
        fp16=True if device == "cuda" else False,
        bf16=False,
        lr_scheduler_type="cosine",
        weight_decay=0.01,
        max_grad_norm=1.0,
        seed=42,
        dataloader_num_workers=4,
        remove_unused_columns=False,
    )
    
    # Trainer
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        compute_metrics=lambda pred: compute_metrics_qat(pred, processor),
        tokenizer=processor.feature_extractor,
    )
    
    # Entra√Ænement
    print(f"üöÄ D√©marrage entra√Ænement QAT ({args.quantization_type})...")
    print(f"   √âchantillons: {len(train_dataset)}")
    print(f"   √âpoques: {args.num_epochs}")
    print(f"   Batch size: {args.per_device_batch_size}")
    print(f"   Gradient accumulation: {args.gradient_accumulation_steps}")
    print(f"   Learning rate: {args.learning_rate}")
    print()
    
    trainer.train()
    
    # Sauvegarder mod√®le final
    final_output_dir = Path(args.output_dir) / "final"
    final_output_dir.mkdir(parents=True, exist_ok=True)
    
    trainer.save_model(str(final_output_dir))
    processor.save_pretrained(str(final_output_dir))
    
    print(f"‚úÖ Mod√®le QAT sauvegard√©: {final_output_dir}")
    
    # √âvaluation finale
    print("\nüìä √âvaluation finale...")
    eval_results = trainer.evaluate()
    print(f"WER final: {eval_results.get('eval_wer', 'N/A'):.2%}")
    
    print(f"\nüí° Prochaine √©tape: Convertir en mod√®le quantifi√©")
    print(f"   python scripts/convert_qat_to_quantized.py \\")
    print(f"     --model_path {final_output_dir} \\")
    print(f"     --output_path {args.output_dir}-quantized \\")
    print(f"     --quantization_type {args.quantization_type}")


if __name__ == "__main__":
    main()

