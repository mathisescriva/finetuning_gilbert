# R√©sultats de Benchmark - Whisper-Large-V3-Distil-French-v0.2

## üìä Configuration

- **Mod√®le**: `bofenghuang/whisper-large-v3-distil-fr-v0.2`
- **GPU**: NVIDIA GeForce RTX 5090
- **Format**: FP16 (float16)
- **Date**: 2025-12-08

## üéØ M√©triques de Performance

### Taille du Mod√®le

- **Param√®tres**: 756.4M (millions)
- **Taille sur disque**: 1.51 GB (FP16)
- **Format**: PyTorch float16

### Vitesse d'Inf√©rence

- **Latence moyenne**: 0.053s ¬± 0.082s pour 30 secondes d'audio
- **Temps m√©dian**: ~0.03s (estim√©)
- **D√©bit**: **569x temps r√©el**
  - Signifie: peut transcrire 569 secondes (9.5 minutes) d'audio en 1 seconde r√©elle

### M√©moire

- **Utilisation VRAM**: 0.06 GB (mesure diff√©rentielle)
- **Note**: L'utilisation totale incluant le mod√®le charg√© est d'environ 1.57 GB (mod√®le + overhead)

## üìà Comparaison avec Baseline (Whisper Large-v3)

### Bas√© sur la documentation du mod√®le distill√©:

| M√©trique | Whisper Large-v3 | Distil-French v0.2 | Am√©lioration |
|----------|------------------|-------------------|--------------|
| **Taille** | ~3.0 GB | 1.51 GB | **-50%** |
| **Param√®tres** | ~1.5B | 756.4M | **-49%** |
| **Vitesse** | Baseline | **5.8x plus rapide** | **5.8x** |
| **Qualit√© (WER)** | R√©f√©rence | +1-2% WER | **Minimal** |

### M√©triques Mesur√©es (Notre Benchmark)

- **Vitesse**: 569x temps r√©el (mesur√©)
- **Efficacit√© m√©moire**: 1.57 GB VRAM total (tr√®s frugal)
- **D√©bit**: ~18,000 secondes d'audio par heure r√©elle

## üéì M√©triques pour Publication

### R√©sum√© Ex√©cutif

Le mod√®le **Whisper-Large-V3-Distil-French-v0.2** offre un excellent compromis qualit√©/performance :

1. **Frugalit√©**: 
   - 50% plus petit que le mod√®le complet
   - M√©moire VRAM r√©duite √† 1.57 GB
   - Adapt√© pour d√©ploiement on-premise et edge

2. **Performance**:
   - 5.8x plus rapide que Whisper Large-v3
   - D√©bit de 569x temps r√©el sur RTX 5090
   - Latence < 0.1s pour 30s d'audio

3. **Qualit√©**:
   - D√©gradation minimale (+1-2% WER selon documentation)
   - Sp√©cialis√© pour le fran√ßais
   - Optimis√© pour transcription longue dur√©e

### Points Cl√©s pour le Papier

- **Innovation**: Distillation sp√©cialis√©e fran√ßais avec encodeur pr√©serv√©
- **Efficacit√©**: 50% r√©duction taille, 5.8x acc√©l√©ration
- **Utilit√©**: D√©ploiement edge/on-premise possible (16GB GPU suffisant)
- **Robustesse**: Moins d'hallucinations en long-form que le mod√®le complet

## üìã M√©triques Compl√©mentaires (√† Mesurer)

Pour une publication compl√®te, mesurer √©galement :

- [ ] **WER/CER** sur datasets de test (Common Voice, MLS)
- [ ] **Latence** sur diff√©rents GPU (RTX 3090, A100, CPU)
- [ ] **Comparaison qualit√©** vs baseline sur m√™mes datasets
- [ ] **M√©triques sp√©cialis√©es**: noms propres, acronymes, termes m√©tier
- [ ] **Temps d'inf√©rence** par minute d'audio
- [ ] **M√©moire RAM** en plus de VRAM

## üíæ Utilisation

```python
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor
import torch

model_name = "bofenghuang/whisper-large-v3-distil-fr-v0.2"
processor = AutoProcessor.from_pretrained(model_name)
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True
)
model.to("cuda")

# Transcription
inputs = processor(audio, sampling_rate=16000, return_tensors="pt")
generated_ids = model.generate(**inputs.to("cuda"), language="fr")
transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

## üìù Notes pour le Papier

- Mentionner que les m√©triques sont mesur√©es sur RTX 5090
- Le d√©bit varie selon la longueur d'audio et GPU
- Pour CPU, utiliser quantization (int8) pour meilleure performance
- Le mod√®le est compatible avec faster-whisper, whisper.cpp pour optimisations suppl√©mentaires

